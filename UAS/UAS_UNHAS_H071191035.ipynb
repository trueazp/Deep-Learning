{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLhmp35geRVj"
      },
      "source": [
        "<H1><b>UAS Universitas Hasanuddin - CycleGAN Implementation Using Tensforflow</b></H1>\n",
        "\n",
        "<H3>Nama : Akmal Zuhdy Prasetya</H3>\n",
        "<H3>NIM  : H071191035</H3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbaRNu_LZM5W"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaFzQHj-XdMx"
      },
      "source": [
        "Let's start with importing our standard libraries here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQ5q7Tb2KVP",
        "outputId": "a8afec26-c584-483f-f068-f18855aa658b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scipy==1.2.0\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hsXuV39I1jYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_examples\n",
        "import numpy as np\n",
        "from scipy.misc import imsave\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import time\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSXnAbC-_FGW",
        "outputId": "e7b726bf-7efa-4542-fa49-6e63352af555"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VynBpQhn4bmw"
      },
      "source": [
        "# **Defining Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eO8QCizf4r7I"
      },
      "outputs": [],
      "source": [
        "def lrelu(x, leak=0.2, name=\"lrelu\", alt_relu_impl=False):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        if alt_relu_impl:\n",
        "            f1 = 0.5 * (1 + leak)\n",
        "            f2 = 0.5 * (1 - leak)\n",
        "            # lrelu = 1/2 * (1 + leak) * x + 1/2 * (1 - leak) * |x|\n",
        "            return f1 * x + f2 * abs(x)\n",
        "        else:\n",
        "            return tf.maximum(x, leak*x)\n",
        "\n",
        "def instance_norm(x):\n",
        "\n",
        "    with tf.variable_scope(\"instance_norm\"):\n",
        "        epsilon = 1e-5\n",
        "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
        "        scale = tf.get_variable('scale',[x.get_shape()[-1]], \n",
        "            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
        "        offset = tf.get_variable('offset',[x.get_shape()[-1]],initializer=tf.constant_initializer(0.0))\n",
        "        out = scale*tf.div(x-mean, tf.sqrt(var+epsilon)) + offset\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def general_conv2d(inputconv, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"conv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
        "    with tf.variable_scope(name):\n",
        "        \n",
        "        conv = tf.compat.v1.layers.conv2d(inputconv, o_d, f_w, s_w, padding, kernel_initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        # conv = tf.compat.v1.layers.conv2d(inputconv, o_d, f_w, s_w, padding, activation_fn=tf.nn.relu, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
        "        if do_norm:\n",
        "            conv = instance_norm(conv)\n",
        "            # conv = tf.compat.v1.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
        "            \n",
        "        if do_relu:\n",
        "            if(relufactor == 0):\n",
        "                conv = tf.nn.relu(conv,\"relu\")\n",
        "            else:\n",
        "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "\n",
        "def general_deconv2d(inputconv, outshape, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"deconv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        conv = tf.compat.v1.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, kernel_initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        # conv = tf.compat.v1.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=tf.nn.relu, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
        "        \n",
        "        if do_norm:\n",
        "            conv = instance_norm(conv)\n",
        "            # conv = tf.compat.v1.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
        "            \n",
        "        if do_relu:\n",
        "            if(relufactor == 0):\n",
        "                conv = tf.nn.relu(conv,\"relu\")\n",
        "            else:\n",
        "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
        "\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdl4m3Wa4eMA"
      },
      "source": [
        "# **Defining Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7YKSfO_U4uh6"
      },
      "outputs": [],
      "source": [
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "ngf = 32\n",
        "ndf = 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_resnet_block(inputres, dim, name=\"resnet\"):\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c1\")\n",
        "        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c2\",do_relu=False)\n",
        "        \n",
        "        return tf.nn.relu(out_res + inputres)\n",
        "\n",
        "\n",
        "def build_generator_resnet_6blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r6, [batch_size,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c6 = general_conv2d(o_c5_pad, img_layer, f, f, 1, 1, 0.02,\"VALID\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "def build_generator_resnet_9blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "        o_r7 = build_resnet_block(o_r6, ngf*4, \"r7\")\n",
        "        o_r8 = build_resnet_block(o_r7, ngf*4, \"r8\")\n",
        "        o_r9 = build_resnet_block(o_r8, ngf*4, \"r9\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r9, [batch_size,128,128,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,256,256,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c6 = general_conv2d(o_c5, img_layer, f, f, 1, 1, 0.02,\"SAME\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "\n",
        "def build_gen_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f = 4\n",
        "\n",
        "        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=False, relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, \"SAME\", \"c4\",relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5\n",
        "\n",
        "\n",
        "def patch_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f= 4\n",
        "\n",
        "        patch_input = tf.random_crop(inputdisc,[1,70,70,3])\n",
        "        o_c1 = general_conv2d(patch_input, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=\"False\", relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 2, 2, 0.02, \"SAME\", \"c4\", relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH0vx9Qk6CdR"
      },
      "source": [
        "# **Main Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "7Zcm7r1H6CnQ",
        "outputId": "59839228-821f-4626-fd3e-e33a2affadda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-c29fbf5a63a7>:46: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-6-c29fbf5a63a7>:49: WholeFileReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.map(tf.read_file)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c29fbf5a63a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-c29fbf5a63a7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mto_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c29fbf5a63a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m#Build the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m#Loss function calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c29fbf5a63a7>\u001b[0m in \u001b[0;36mmodel_setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator_resnet_9blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"g_A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator_resnet_9blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"g_B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_gen_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d_A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f237480b6f7f>\u001b[0m in \u001b[0;36mbuild_generator_resnet_9blocks\u001b[0;34m(inputgen, name)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mo_c4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneral_deconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_r9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mo_c5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneral_deconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_c4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mo_c6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneral_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_c5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_relu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6484201bf960>\u001b[0m in \u001b[0;36mgeneral_deconv2d\u001b[0;34m(inputconv, outshape, o_d, f_h, f_w, s_h, s_w, stddev, padding, name, do_norm, do_relu, relufactor)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# conv = tf.compat.v1.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=tf.nn.relu, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m   1734\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m       _scope=name)\n\u001b[0;32m-> 1736\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0;34m'Please use `layer.__call__` method instead.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 1683\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_doc_inheritable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m           with autocast_variable.enable_auto_cast_variables(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2056\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2057\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs to a layer should be tensors. Got: {x}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: None"
          ]
        }
      ],
      "source": [
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "to_train = True\n",
        "to_test = False\n",
        "to_restore = False\n",
        "output_path = \"./output\"\n",
        "check_dir = \"./output/checkpoints/\"\n",
        "\n",
        "\n",
        "temp_check = 0\n",
        "\n",
        "\n",
        "\n",
        "max_epoch = 1\n",
        "max_images = 100\n",
        "\n",
        "h1_size = 150\n",
        "h2_size = 300\n",
        "z_size = 100\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "sample_size = 10\n",
        "save_training_images = True\n",
        "ngf = 32\n",
        "ndf = 64\n",
        "\n",
        "class CycleGAN():\n",
        "\n",
        "    def input_setup(self):\n",
        "\n",
        "        ''' \n",
        "        This function basically setup variables for taking image input.\n",
        "        filenames_A/filenames_B -> takes the list of all training images\n",
        "        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\n",
        "        '''\n",
        "\n",
        "        filenames_A = tf.train.match_filenames_once(\"./input/horse2zebra/trainA/*.jpg\")    \n",
        "        self.queue_length_A = tf.size(filenames_A)\n",
        "        filenames_B = tf.train.match_filenames_once(\"./input/horse2zebra/trainB/*.jpg\")    \n",
        "        self.queue_length_B = tf.size(filenames_B)\n",
        "        \n",
        "        filename_queue_A = tf.train.string_input_producer(filenames_A)\n",
        "        filename_queue_B = tf.train.string_input_producer(filenames_B)\n",
        "\n",
        "        image_reader = tf.WholeFileReader()\n",
        "        _, image_file_A = image_reader.read(filename_queue_A)\n",
        "        _, image_file_B = image_reader.read(filename_queue_B)\n",
        "\n",
        "        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A),[256,256]),127.5),1)\n",
        "        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B),[256,256]),127.5),1)\n",
        "\n",
        "    \n",
        "\n",
        "    def input_read(self, sess):\n",
        "\n",
        "\n",
        "        '''\n",
        "        It reads the input into from the image folder.\n",
        "        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\n",
        "        self.A_input/self.B_input -> Stores all the training images in python list\n",
        "        '''\n",
        "\n",
        "        # Loading images into the tensors\n",
        "        coord = tf.train.Coordinator()\n",
        "        threads = tf.train.start_queue_runners(coord=coord)\n",
        "\n",
        "        num_files_A = sess.run(self.queue_length_A)\n",
        "        num_files_B = sess.run(self.queue_length_B)\n",
        "\n",
        "        self.fake_images_A = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "        self.fake_images_B = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        self.A_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "        self.B_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images): \n",
        "            image_tensor = sess.run(self.image_A)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.A_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images):\n",
        "            image_tensor = sess.run(self.image_B)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.B_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        coord.request_stop()\n",
        "        coord.join(threads)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def model_setup(self):\n",
        "\n",
        "        ''' This function sets up the model to train\n",
        "        self.input_A/self.input_B -> Set of training images.\n",
        "        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\n",
        "        self.lr -> Learning rate variable\n",
        "        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. This is use to calcualte cyclic loss\n",
        "        '''\n",
        "\n",
        "        self.input_A = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_A\")\n",
        "        self.input_B = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_B\")\n",
        "        \n",
        "        self.fake_pool_A = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_A\")\n",
        "        self.fake_pool_B = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_B\")\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "        self.num_fake_inputs = 0\n",
        "\n",
        "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
        "\n",
        "        with tf.variable_scope(\"Model\") as scope:\n",
        "            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=\"g_A\")\n",
        "            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=\"g_B\")\n",
        "            self.rec_A = build_gen_discriminator(self.input_A, \"d_A\")\n",
        "            self.rec_B = build_gen_discriminator(self.input_B, \"d_B\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_rec_A = build_gen_discriminator(self.fake_A, \"d_A\")\n",
        "            self.fake_rec_B = build_gen_discriminator(self.fake_B, \"d_B\")\n",
        "            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, \"g_B\")\n",
        "            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, \"g_A\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, \"d_A\")\n",
        "            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, \"d_B\")\n",
        "\n",
        "    def loss_calc(self):\n",
        "\n",
        "        ''' In this function we are defining the variables for loss calcultions and traning model\n",
        "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
        "        g_loss_A/g_loss_B -> loss for generator A/B\n",
        "        *_trainer -> Variaous trainer for above loss functions\n",
        "        *_summ -> Summary variables for above loss functions'''\n",
        "\n",
        "        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\n",
        "        \n",
        "        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))\n",
        "        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))\n",
        "        \n",
        "        g_loss_A = cyc_loss*10 + disc_loss_B\n",
        "        g_loss_B = cyc_loss*10 + disc_loss_A\n",
        "\n",
        "        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1)))/2.0\n",
        "        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1)))/2.0\n",
        "\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n",
        "\n",
        "        self.model_vars = tf.trainable_variables()\n",
        "\n",
        "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
        "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
        "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
        "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
        "        \n",
        "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
        "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
        "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
        "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
        "\n",
        "        for var in self.model_vars: print(var.name)\n",
        "\n",
        "        #Summary variables for tensorboard\n",
        "\n",
        "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
        "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
        "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
        "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
        "\n",
        "    def save_training_images(self, sess, epoch):\n",
        "\n",
        "        if not os.path.exists(\"./output/imgs\"):\n",
        "            os.makedirs(\"./output/imgs\")\n",
        "\n",
        "        for i in range(0,10):\n",
        "            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "            imsave(\"./output/imgs/fakeB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/fakeA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
        "        ''' This function saves the generated image to corresponding pool of images.\n",
        "        In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
        "        already stored image and replace it with new one.'''\n",
        "\n",
        "        if(num_fakes < pool_size):\n",
        "            fake_pool[num_fakes] = fake\n",
        "            return fake\n",
        "        else :\n",
        "            p = random.random()\n",
        "            if p > 0.5:\n",
        "                random_id = random.randint(0,pool_size-1)\n",
        "                temp = fake_pool[random_id]\n",
        "                fake_pool[random_id] = fake\n",
        "                return temp\n",
        "            else :\n",
        "                return fake\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "\n",
        "        ''' Training Function '''\n",
        "\n",
        "\n",
        "        # Load Dataset from the dataset folder\n",
        "        self.input_setup()  \n",
        "\n",
        "        #Build the network\n",
        "        self.model_setup()\n",
        "\n",
        "        #Loss function calculations\n",
        "        self.loss_calc()\n",
        "      \n",
        "        # Initializing the global variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()     \n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            #Read input to nd array\n",
        "            self.input_read(sess)\n",
        "\n",
        "            #Restore the model to run the model from last checkpoint\n",
        "            if to_restore:\n",
        "                chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "                saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            writer = tf.summary.FileWriter(\"./output/2\")\n",
        "\n",
        "            if not os.path.exists(check_dir):\n",
        "                os.makedirs(check_dir)\n",
        "\n",
        "            # Training Loop\n",
        "            for epoch in range(sess.run(self.global_step),100):                \n",
        "                print (\"In the epoch \", epoch)\n",
        "                saver.save(sess,os.path.join(check_dir,\"cyclegan\"),global_step=epoch)\n",
        "\n",
        "                # Dealing with the learning rate as per the epoch number\n",
        "                if(epoch < 100) :\n",
        "                    curr_lr = 0.0002\n",
        "                else:\n",
        "                    curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n",
        "\n",
        "                if(save_training_images):\n",
        "                    self.save_training_images(sess, epoch)\n",
        "\n",
        "                # sys.exit()\n",
        "\n",
        "                for ptr in range(0,max_images):\n",
        "                    print(\"In the iteration \",ptr)\n",
        "                    print(\"Starting\",time.time()*1000.0)\n",
        "\n",
        "                    # Optimizing the G_A network\n",
        "\n",
        "                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "                    \n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)                    \n",
        "                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
        "                    \n",
        "                    # Optimizing the D_B network\n",
        "                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_B:fake_B_temp1})\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    # Optimizing the G_B network\n",
        "                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
        "\n",
        "                    # Optimizing the D_A network\n",
        "                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    self.num_fake_inputs+=1\n",
        "            \n",
        "                        \n",
        "\n",
        "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
        "\n",
        "            writer.add_graph(sess.graph)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "\n",
        "        ''' Testing Function'''\n",
        "\n",
        "        print(\"Testing the results\")\n",
        "\n",
        "        self.input_setup()\n",
        "\n",
        "        self.model_setup()\n",
        "        saver = tf.train.Saver()\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            self.input_read(sess)\n",
        "\n",
        "            chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "            saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            if not os.path.exists(\"./output/imgs/test/\"):\n",
        "                os.makedirs(\"./output/imgs/test/\")            \n",
        "\n",
        "            for i in range(0,100):\n",
        "                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "                imsave(\"./output/imgs/test/fakeB_\"+str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/fakeA_\"+str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputA_\"+str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputB_\"+str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    model = CycleGAN()\n",
        "    if to_train:\n",
        "        model.train()\n",
        "    elif to_test:\n",
        "        model.test()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnhDd9P7nPmT"
      },
      "source": [
        "# **Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR9DDkPfnVAp"
      },
      "source": [
        "Transferring characteristics from one image to another is an exciting proposition. How cool would it be if we could take a photo and convert it into the style of Van Gogh or Picasso!\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1M38cToPbicUJ1Q3Yg9gbN0VYuhti7Zt4\" width=\"500px\"/></center>\n",
        "\n",
        "Or maybe we want to put a smile on Agent 42's face with the virally popular Faceapp.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1H-iM78_FbUxqy_4YqNeWeYerj0T5SVaQ\" width=\"500px\"/></center>\n",
        "\n",
        "These are examples of cross domain image transfer, we want to take an image from an input domain $D_i$ Di and then transform it into an image of target domain $D_t$ without necessarily having a one-to-one mapping between images from input to target domain in the training set. Relaxation of having one-to-one mapping makes this formulation quite powerful the same method could be used to tackle a variety of problems by varying the input-output domain pairs performing artistic style transfer, adding bokeh effect to phone camera photos, creating outline maps from satellite images or convert horses to zebras and vice versa!! This is achieved by a type of generative model, specifically a Generative Adversarial Network dubbed CycleGAN. Here are some examples of what CycleGAN can do.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1TPHv_NEGYYOvVlI3V7PlgmZkJtWJIFk1\" width=\"500px\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Wdw6o7qYaH"
      },
      "source": [
        "We are not going to go look at GANs from scratch, check out this simplified tutorial to get a hang of it. This workshop video at NIPS 2016 by Ian Goodfellow (the guy behind the GANs) is also a great resource. What we will be doing in this post is look at how to implement a CycleGAN in Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLnCRTI5qoIh"
      },
      "source": [
        "## **1. Unpaired Image-to-Image Translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pj22syHq__M"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1juJUU5JjXSNaaKKIwJt3tzS4u0Z_HOa2\" width=\"500px\"/></center>\n",
        "\n",
        "As mentioned earlier, the CycleGAN works without paired examples of transformation from source to target domain. Recent methods such as Pix2Pix depend on the availaibilty of training examples where the same data is available in both domains. The power of CycleGAN lies in being able to learn such transformations without one-to-one mapping between training data in source and target domains. The need for a paired image in the target domain is eliminated by making a two-step transformation of source domain image - first by trying to map it to target domain and then back to the original image. Mapping the image to target domain is done using a generator network and the quality of this generated image is improved by pitching the generator against a discrimintor (as described below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbVdD3jmrfwX"
      },
      "source": [
        "### **1.1 Adversarial Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-YGuGg4rk9l"
      },
      "source": [
        "We have a generator network and discriminator network playing against each other. The generator tries to produce samples from the desired distribution and the discriminator tries to predict if the sample is from the actual distribution or produced by the generator. The generator and discriminator are trained jointly. The effect this has is that eventually the generator learns to approximate the underlying distribution completely and the discriminator is left guessing randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ_yYTKErpDG"
      },
      "source": [
        "### **1.2 Cycle-Consistent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXzejJArwSa"
      },
      "source": [
        " The above adversarial method of training has a problem though. Quoting the authors of the original paper:\n",
        "\n",
        "\"Adversarial training can, in theory, learn mappings $G$ and $F$ that produce outputs identically distributed as target domains $Y$ and $X$ respectively. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. Thus, an adversarial loss alone cannot guarantee that the learned function can map an individual input $x_i$ to a desired output $y_i$.\"\n",
        "\n",
        "To regularize the model, the authors introduce the constraint of cycle-consistency if we transform from source distribution to target and then back again to source distribution, we should get samples from our source distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHJog692sNrO"
      },
      "source": [
        "## **2. Network Architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SHxUGWuAmH"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1ybmuApppLEgsADnNk0SYnu63XB7iN7oS\" width=\"500px\"/></center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=13PPQBt43koR4juMB53XYgSe4Bx5mKSGE\" width=\"500px\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJIzZN5Munp7"
      },
      "source": [
        "In a paired dataset, every image, say $img_A$, is manually mapped to some image, say $img_B$, in target domain, such that they share various features. Features that can be used to map an image $(img_A/img_B)$ to its correspondingly mapped counterpart $(img_B/img_A)$. Basically, pairing is done to make input and output share some common features. This mapping defines meaningful transformation of an image from one damain to another domain. So, when we have paired dataset, generator must take an input, say $input_A$, from domain $D_A$ and map this image to an output image, say $gen_B$, which must be close to its mapped counterpart. But we don't have this luxury in unpaired dataset, there is no pre-defined meaningful transformation that we can learn, so, we will create it. We need to make sure that there is some meaningful relation between input image and generated image. So, authors tried to enforce this by saying that Generator will map input image $(input_A)$ from domain $D_A$ to some image in target domain $D_B$, but to make sure that there is meaningful relation between these images, they must share some feature, features that can be used to map this output image back to input image, so there must be another generator that must be able to map back this output image back to original input. So, you can see this condition defining a meaningful mapping between $input_A$ and $gen_B$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03qcUM6LvLoO"
      },
      "source": [
        "In a nutshell, the model works by taking an input image from domain $D_A$ which is fed to our first generator $Generator_A\\to_B$ whose job is to transform a given image from domain $D_A$ to an image in target domain $D_B$. This new generated image is then fed to another generator $Generator_B\\to_A$ which converts it back into an image, $Cyclic_A$, from our original domain $D_A$ (think of autoencoders, except that our latent space is $D_t$). And as we discussed in above paragraph, this output image must be close to original input image to define a meaningful mapping that is absent in unpaired dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFGnkfWwsTZ"
      },
      "source": [
        "As we can see in above figure, two inputs are fed into each discriminator(one is original image corresponding to that domain and other is the generated image via a generator) and the job of discriminator is to distinguish between them, so that discriminator is able to defy the adversary (in this case generator) and reject images generated by it. While the generator would like to make sure that these images get accepted by the discriminator, so it will try to generate images which are very close to original images in Class $D_B$. (In fact, the generator and discriminator are actually playing a game whose Nash equilibrium is achieved when the generator's distribution becomes same as the desired distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p69H3YdIw1VL"
      },
      "source": [
        "## **2. Building Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jV8N-m0w5C-"
      },
      "source": [
        "High level structure of Generator can be viewed in the following image.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1ybmuApppLEgsADnNk0SYnu63XB7iN7oS\" width=\"500px\"/></center>\n",
        "\n",
        "The generator have three components:\n",
        "1. Encoder\n",
        "2. Transformer\n",
        "3. Decoder \n",
        "\n",
        "Following are the parameters we have used for the mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCGxpbSQxMcI"
      },
      "outputs": [],
      "source": [
        "ngf = 32 # Number of filters in first layer of generator\n",
        "ndf = 64 # Number of filters in first layer of discriminator\n",
        "batch_size = 1 # batch_size\n",
        "pool_size = 50 # pool_size\n",
        "img_width = 256 # Imput image will of width 256\n",
        "img_height = 256 # Input image will be of height 256\n",
        "img_depth = 3 # RGB format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681JzMUixPlt"
      },
      "source": [
        "First three parameters are self explanatory and we will explain what `pool_size` means in the **Generated Image Pool** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geaH50nUxbXJ"
      },
      "source": [
        "## **3. Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeSGZHHJywkH"
      },
      "source": [
        "For the purpose of simplicity, throughout the article we will assume that the input size is $[256,256,3]$. The first step is extracting the features from an image which is done a convolution network. As input a convolution network takes an image, size of filter window that we move over input image to extract out features and the stride size to decide how much we will move filter window after each step. So the first layer of encoding looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5ZpMKOd1ZDz"
      },
      "outputs": [],
      "source": [
        "o_c1 = general_conv2d(input_gen, num_features=ngf, window_width=7, window_height=7, stride_width=1, stride_height=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxnfpWsmxfSt"
      },
      "source": [
        "## **4. Transformation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGFAHyUyxifi"
      },
      "source": [
        "## **5. Decoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCwJw4iFxlp2"
      },
      "source": [
        "## **6. Building Discriminator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_n4dalxuom"
      },
      "source": [
        "## **7. Building Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie0BEZGcxyWl"
      },
      "source": [
        "## **8. Loss Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTNnzID-x39o"
      },
      "source": [
        "### **8.1 Discriminator Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlA0WIzPx-4f"
      },
      "source": [
        "### **8.2 Generator Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0u1wNSRyB9C"
      },
      "source": [
        "### **8.3 Cyclic Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRVmGr3uyFLg"
      },
      "source": [
        "### **8.4 All Losses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ihyev6yTro"
      },
      "source": [
        "## **9. Training Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv2WuJ41ydLD"
      },
      "source": [
        "### **9.1 Generated Image Pool**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc3dOT2RyZ3C"
      },
      "source": [
        "## **10. Result**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "UAS-UNHAS-H071191035.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}