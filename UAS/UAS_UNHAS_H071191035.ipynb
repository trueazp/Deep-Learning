{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLhmp35geRVj"
      },
      "source": [
        "<H1><b>UAS Universitas Hasanuddin - CycleGAN Implementation Using Tensforflow</b></H1>\n",
        "\n",
        "<H3>Nama : Akmal Zuhdy Prasetya</H3>\n",
        "<H3>NIM  : H071191035</H3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbaRNu_LZM5W"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaFzQHj-XdMx"
      },
      "source": [
        "Let's start with importing our standard libraries here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQ5q7Tb2KVP",
        "outputId": "6375da21-39fe-49ed-8341-0389e3983f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scipy==1.2.0\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hsXuV39I1jYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_examples\n",
        "import numpy as np\n",
        "from scipy.misc import imsave\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import time\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSXnAbC-_FGW",
        "outputId": "97b67bf6-53d0-44ed-c16d-be14ab0b7d41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VynBpQhn4bmw"
      },
      "source": [
        "# **Defining Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eO8QCizf4r7I"
      },
      "outputs": [],
      "source": [
        "def lrelu(x, leak=0.2, name=\"lrelu\", alt_relu_impl=False):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        if alt_relu_impl:\n",
        "            f1 = 0.5 * (1 + leak)\n",
        "            f2 = 0.5 * (1 - leak)\n",
        "            # lrelu = 1/2 * (1 + leak) * x + 1/2 * (1 - leak) * |x|\n",
        "            return f1 * x + f2 * abs(x)\n",
        "        else:\n",
        "            return tf.maximum(x, leak*x)\n",
        "\n",
        "def instance_norm(x):\n",
        "\n",
        "    with tf.variable_scope(\"instance_norm\"):\n",
        "        epsilon = 1e-5\n",
        "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
        "        scale = tf.get_variable('scale',[x.get_shape()[-1]], \n",
        "            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
        "        offset = tf.get_variable('offset',[x.get_shape()[-1]],initializer=tf.constant_initializer(0.0))\n",
        "        out = scale*tf.div(x-mean, tf.sqrt(var+epsilon)) + offset\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def general_conv2d(inputconv, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"conv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
        "    with tf.variable_scope(name):\n",
        "        \n",
        "        conv = tf.compat.v1.layers.conv2d(inputconv, o_d, f_w, s_w, padding, kernel_initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        # conv = tf.compat.v1.layers.conv2d(inputconv, o_d, f_w, s_w, padding, activation_fn=tf.nn.relu, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
        "        if do_norm:\n",
        "            conv = instance_norm(conv)\n",
        "            # conv = tf.compat.v1.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
        "            \n",
        "        if do_relu:\n",
        "            if(relufactor == 0):\n",
        "                conv = tf.nn.relu(conv,\"relu\")\n",
        "            else:\n",
        "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "\n",
        "def general_deconv2d(inputconv, outshape, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"deconv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        conv = tf.compat.v1.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, kernel_initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        # conv = tf.compat.v1.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=tf.nn.relu, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
        "        \n",
        "        if do_norm:\n",
        "            conv = instance_norm(conv)\n",
        "            # conv = tf.compat.v1.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
        "            \n",
        "        if do_relu:\n",
        "            if(relufactor == 0):\n",
        "                conv = tf.nn.relu(conv,\"relu\")\n",
        "            else:\n",
        "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
        "\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdl4m3Wa4eMA"
      },
      "source": [
        "# **Defining Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7YKSfO_U4uh6"
      },
      "outputs": [],
      "source": [
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "ngf = 32\n",
        "ndf = 64\n",
        "\n",
        "def build_resnet_block(inputres, dim, name=\"resnet\"):\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c1\")\n",
        "        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c2\",do_relu=False)\n",
        "        \n",
        "        return tf.nn.relu(out_res + inputres)\n",
        "\n",
        "\n",
        "def build_generator_resnet_6blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r6, [batch_size,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c6 = general_conv2d(o_c5_pad, img_layer, f, f, 1, 1, 0.02,\"VALID\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "def build_generator_resnet_9blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "        o_r7 = build_resnet_block(o_r6, ngf*4, \"r7\")\n",
        "        o_r8 = build_resnet_block(o_r7, ngf*4, \"r8\")\n",
        "        o_r9 = build_resnet_block(o_r8, ngf*4, \"r9\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r9, [batch_size,128,128,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,256,256,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c6 = general_conv2d(o_c5, img_layer, f, f, 1, 1, 0.02,\"SAME\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "\n",
        "def build_gen_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f = 4\n",
        "\n",
        "        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=False, relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, \"SAME\", \"c4\",relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5\n",
        "\n",
        "\n",
        "def patch_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f= 4\n",
        "\n",
        "        patch_input = tf.random_crop(inputdisc,[1,70,70,3])\n",
        "        o_c1 = general_conv2d(patch_input, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=\"False\", relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 2, 2, 0.02, \"SAME\", \"c4\", relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH0vx9Qk6CdR"
      },
      "source": [
        "# **Main Code**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "to_train = True\n",
        "to_test = False\n",
        "to_restore = False\n",
        "output_path = \"./output\"\n",
        "check_dir = \"./output/checkpoints/\"\n",
        "\n",
        "temp_check = 0\n",
        "\n",
        "max_epoch = 1\n",
        "max_images = 100\n",
        "\n",
        "h1_size = 150\n",
        "h2_size = 300\n",
        "z_size = 100\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "sample_size = 10\n",
        "save_training_images = True\n",
        "ngf = 32\n",
        "ndf = 64\n",
        "\n",
        "class CycleGAN():\n",
        "\n",
        "    def input_setup(self):\n",
        "\n",
        "        ''' \n",
        "        This function basically setup variables for taking image input.\n",
        "        filenames_A/filenames_B -> takes the list of all training images\n",
        "        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\n",
        "        '''\n",
        "\n",
        "        filenames_A = tf.train.match_filenames_once(\"./input/horse2zebra/trainA/*.jpg\")    \n",
        "        self.queue_length_A = tf.size(filenames_A)\n",
        "        filenames_B = tf.train.match_filenames_once(\"./input/horse2zebra/trainB/*.jpg\")    \n",
        "        self.queue_length_B = tf.size(filenames_B)\n",
        "        \n",
        "        filename_queue_A = tf.train.string_input_producer(filenames_A)\n",
        "        filename_queue_B = tf.train.string_input_producer(filenames_B)\n",
        "\n",
        "        image_reader = tf.WholeFileReader()\n",
        "        _, image_file_A = image_reader.read(filename_queue_A)\n",
        "        _, image_file_B = image_reader.read(filename_queue_B)\n",
        "\n",
        "        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A),[256,256]),127.5),1)\n",
        "        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B),[256,256]),127.5),1)\n",
        "\n",
        "    \n",
        "\n",
        "    def input_read(self, sess):\n",
        "\n",
        "\n",
        "        '''\n",
        "        It reads the input into from the image folder.\n",
        "        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\n",
        "        self.A_input/self.B_input -> Stores all the training images in python list\n",
        "        '''\n",
        "\n",
        "        # Loading images into the tensors\n",
        "        coord = tf.train.Coordinator()\n",
        "        threads = tf.train.start_queue_runners(coord=coord)\n",
        "\n",
        "        num_files_A = sess.run(self.queue_length_A)\n",
        "        num_files_B = sess.run(self.queue_length_B)\n",
        "\n",
        "        self.fake_images_A = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "        self.fake_images_B = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        self.A_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "        self.B_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images): \n",
        "            image_tensor = sess.run(self.image_A)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.A_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images):\n",
        "            image_tensor = sess.run(self.image_B)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.B_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        coord.request_stop()\n",
        "        coord.join(threads)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def model_setup(self):\n",
        "\n",
        "        ''' This function sets up the model to train\n",
        "        self.input_A/self.input_B -> Set of training images.\n",
        "        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\n",
        "        self.lr -> Learning rate variable\n",
        "        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. This is use to calcualte cyclic loss\n",
        "        '''\n",
        "\n",
        "        self.input_A = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_A\")\n",
        "        self.input_B = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_B\")\n",
        "        \n",
        "        self.fake_pool_A = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_A\")\n",
        "        self.fake_pool_B = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_B\")\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "        self.num_fake_inputs = 0\n",
        "\n",
        "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
        "\n",
        "        with tf.variable_scope(\"Model\") as scope:\n",
        "            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=\"g_A\")\n",
        "            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=\"g_B\")\n",
        "            self.rec_A = build_gen_discriminator(self.input_A, \"d_A\")\n",
        "            self.rec_B = build_gen_discriminator(self.input_B, \"d_B\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_rec_A = build_gen_discriminator(self.fake_A, \"d_A\")\n",
        "            self.fake_rec_B = build_gen_discriminator(self.fake_B, \"d_B\")\n",
        "            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, \"g_B\")\n",
        "            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, \"g_A\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, \"d_A\")\n",
        "            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, \"d_B\")\n",
        "\n",
        "    def loss_calc(self):\n",
        "\n",
        "        ''' In this function we are defining the variables for loss calcultions and traning model\n",
        "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
        "        g_loss_A/g_loss_B -> loss for generator A/B\n",
        "        *_trainer -> Variaous trainer for above loss functions\n",
        "        *_summ -> Summary variables for above loss functions'''\n",
        "\n",
        "        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\n",
        "        \n",
        "        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))\n",
        "        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))\n",
        "        \n",
        "        g_loss_A = cyc_loss*10 + disc_loss_B\n",
        "        g_loss_B = cyc_loss*10 + disc_loss_A\n",
        "\n",
        "        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1)))/2.0\n",
        "        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1)))/2.0\n",
        "\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n",
        "\n",
        "        self.model_vars = tf.trainable_variables()\n",
        "\n",
        "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
        "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
        "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
        "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
        "        \n",
        "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
        "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
        "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
        "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
        "\n",
        "        for var in self.model_vars: print(var.name)\n",
        "\n",
        "        #Summary variables for tensorboard\n",
        "\n",
        "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
        "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
        "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
        "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
        "\n",
        "    def save_training_images(self, sess, epoch):\n",
        "\n",
        "        if not os.path.exists(\"./output/imgs\"):\n",
        "            os.makedirs(\"./output/imgs\")\n",
        "\n",
        "        for i in range(0,10):\n",
        "            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "            imsave(\"./output/imgs/fakeB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/fakeA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
        "        ''' This function saves the generated image to corresponding pool of images.\n",
        "        In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
        "        already stored image and replace it with new one.'''\n",
        "\n",
        "        if(num_fakes < pool_size):\n",
        "            fake_pool[num_fakes] = fake\n",
        "            return fake\n",
        "        else :\n",
        "            p = random.random()\n",
        "            if p > 0.5:\n",
        "                random_id = random.randint(0,pool_size-1)\n",
        "                temp = fake_pool[random_id]\n",
        "                fake_pool[random_id] = fake\n",
        "                return temp\n",
        "            else :\n",
        "                return fake\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "\n",
        "        ''' Training Function '''\n",
        "\n",
        "\n",
        "        # Load Dataset from the dataset folder\n",
        "        self.input_setup()  \n",
        "\n",
        "        #Build the network\n",
        "        self.model_setup()\n",
        "\n",
        "        #Loss function calculations\n",
        "        self.loss_calc()\n",
        "      \n",
        "        # Initializing the global variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()     \n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            #Read input to nd array\n",
        "            self.input_read(sess)\n",
        "\n",
        "            #Restore the model to run the model from last checkpoint\n",
        "            if to_restore:\n",
        "                chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "                saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            writer = tf.summary.FileWriter(\"./output/2\")\n",
        "\n",
        "            if not os.path.exists(check_dir):\n",
        "                os.makedirs(check_dir)\n",
        "\n",
        "            # Training Loop\n",
        "            for epoch in range(sess.run(self.global_step),100):                \n",
        "                print (\"In the epoch \", epoch)\n",
        "                saver.save(sess,os.path.join(check_dir,\"cyclegan\"),global_step=epoch)\n",
        "\n",
        "                # Dealing with the learning rate as per the epoch number\n",
        "                if(epoch < 100) :\n",
        "                    curr_lr = 0.0002\n",
        "                else:\n",
        "                    curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n",
        "\n",
        "                if(save_training_images):\n",
        "                    self.save_training_images(sess, epoch)\n",
        "\n",
        "                # sys.exit()\n",
        "\n",
        "                for ptr in range(0,max_images):\n",
        "                    print(\"In the iteration \",ptr)\n",
        "                    print(\"Starting\",time.time()*1000.0)\n",
        "\n",
        "                    # Optimizing the G_A network\n",
        "\n",
        "                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "                    \n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)                    \n",
        "                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
        "                    \n",
        "                    # Optimizing the D_B network\n",
        "                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_B:fake_B_temp1})\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    # Optimizing the G_B network\n",
        "                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
        "\n",
        "                    # Optimizing the D_A network\n",
        "                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    self.num_fake_inputs+=1\n",
        "            \n",
        "                        \n",
        "\n",
        "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
        "\n",
        "            writer.add_graph(sess.graph)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "\n",
        "        ''' Testing Function'''\n",
        "\n",
        "        print(\"Testing the results\")\n",
        "\n",
        "        self.input_setup()\n",
        "\n",
        "        self.model_setup()\n",
        "        saver = tf.train.Saver()\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            self.input_read(sess)\n",
        "\n",
        "            chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "            saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            if not os.path.exists(\"./output/imgs/test/\"):\n",
        "                os.makedirs(\"./output/imgs/test/\")            \n",
        "\n",
        "            for i in range(0,100):\n",
        "                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "                imsave(\"./output/imgs/test/fakeB_\"+str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/fakeA_\"+str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputA_\"+str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputB_\"+str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    model = CycleGAN()\n",
        "    if to_train:\n",
        "        model.train()\n",
        "    elif to_test:\n",
        "        model.test()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "usOqCiRP6Zus"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "UAS-UNHAS-H071191035.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}